{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:03.958735Z",
     "iopub.status.busy": "2024-12-20T10:10:03.958418Z",
     "iopub.status.idle": "2024-12-20T10:10:14.988434Z",
     "shell.execute_reply": "2024-12-20T10:10:14.987314Z",
     "shell.execute_reply.started": "2024-12-20T10:10:03.958697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:14.990970Z",
     "iopub.status.busy": "2024-12-20T10:10:14.990685Z",
     "iopub.status.idle": "2024-12-20T10:10:15.004836Z",
     "shell.execute_reply": "2024-12-20T10:10:15.004030Z",
     "shell.execute_reply.started": "2024-12-20T10:10:14.990943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CHARS = ['_', '^', '{', '}', '&', '\\\\\\\\', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
    "             's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\\\mathbb{A}', '\\\\mathbb{B}',\n",
    "             '\\\\mathbb{C}', '\\\\mathbb{D}', '\\\\mathbb{E}', '\\\\mathbb{F}', '\\\\mathbb{G}', '\\\\mathbb{H}', '\\\\mathbb{I}', '\\\\mathbb{J}',\n",
    "             '\\\\mathbb{K}', '\\\\mathbb{L}', '\\\\mathbb{M}', '\\\\mathbb{N}', '\\\\mathbb{O}', '\\\\mathbb{P}', '\\\\mathbb{Q}', '\\\\mathbb{R}',\n",
    "             '\\\\mathbb{S}', '\\\\mathbb{T}', '\\\\mathbb{U}', '\\\\mathbb{V}', '\\\\mathbb{W}', '\\\\mathbb{X}', '\\\\mathbb{Y}', '\\\\mathbb{Z}',\n",
    "             '\\\\mathbb', ',', ';', ':', '!', '?', '.', '(', ')', '[', ']', '\\\\{', '\\\\}', '*', '/', '+', '-', '\\\\_', '\\\\&', '\\\\#', '\\\\%', '|',\n",
    "             '\\\\backslash', '\\\\alpha', '\\\\beta', '\\\\delta', '\\\\Delta', '\\\\epsilon', '\\\\eta', '\\\\chi', '\\\\gamma', '\\\\Gamma', '\\\\iota',\n",
    "             '\\\\kappa', '\\\\lambda', '\\\\Lambda', '\\\\nu', '\\\\mu', '\\\\omega', '\\\\Omega', '\\\\phi', '\\\\Phi', '\\\\pi', '\\\\Pi', '\\\\psi', '\\\\Psi',\n",
    "             '\\\\rho', '\\\\sigma', '\\\\Sigma', '\\\\tau', '\\\\theta', '\\\\Theta', '\\\\upsilon', '\\\\Upsilon', '\\\\varphi', '\\\\varpi', '\\\\varsigma',\n",
    "             '\\\\vartheta', '\\\\xi', '\\\\Xi', '\\\\zeta', '\\\\frac', '\\\\sqrt', '\\\\prod', '\\\\sum', '\\\\iint', '\\\\int', '\\\\oint', '\\\\hat', '\\\\tilde',\n",
    "             '\\\\vec', '\\\\overline', '\\\\underline', '\\\\prime', '\\\\dot', '\\\\not', '\\\\begin{matrix}', '\\\\end{matrix}', '\\\\langle', '\\\\rangle',\n",
    "             '\\\\lceil', '\\\\rceil', '\\\\lfloor', '\\\\rfloor', '\\\\|', '\\\\ge', '\\\\gg', '\\\\le', '\\\\ll', '<', '>', '=', '\\\\approx', '\\\\cong', '\\\\equiv',\n",
    "             '\\\\ne', '\\\\propto', '\\\\sim', '\\\\simeq', '\\\\in', '\\\\ni', '\\\\notin', '\\\\sqsubseteq', '\\\\subset', '\\\\subseteq', '\\\\subsetneq',\n",
    "             '\\\\supset', '\\\\supseteq', '\\\\emptyset', '\\\\times', '\\\\bigcap', '\\\\bigcirc', '\\\\bigcup', '\\\\bigoplus', '\\\\bigvee', '\\\\bigwedge',\n",
    "             '\\\\cap', '\\\\cup', '\\\\div', '\\\\mp', '\\\\odot', '\\\\ominus', '\\\\oplus', '\\\\otimes', '\\\\pm', '\\\\vee', '\\\\wedge', '\\\\hookrightarrow',\n",
    "             '\\\\leftarrow', '\\\\leftrightarrow', '\\\\Leftrightarrow', '\\\\longrightarrow', '\\\\mapsto', '\\\\rightarrow', '\\\\Rightarrow',\n",
    "             '\\\\rightleftharpoons', '\\\\iff', '\\\\bullet', '\\\\cdot', '\\\\circ', '\\\\aleph', '\\\\angle', '\\\\dagger', '\\\\exists', '\\\\forall',\n",
    "             '\\\\hbar', '\\\\infty', '\\\\models', '\\\\nabla', '\\\\neg', '\\\\partial', '\\\\perp', '\\\\top', '\\\\triangle', '\\\\triangleleft',\n",
    "             '\\\\triangleq', '\\\\vdash', '\\\\Vdash', '\\\\vdots']  \n",
    "CHAR2LABEL = {char: i + 1 for i, char in enumerate(CHARS)}\n",
    "LABEL2CHAR = {label: char for char, label in CHAR2LABEL.items()}\n",
    "_COMMAND_RE = re.compile(r'\\\\(mathbb{[a-zA-Z]}|begin{[a-z]+}|end{[a-z]+}|operatorname\\*|[a-zA-Z]+|.)')\n",
    "\n",
    "def tokenize_expression(s: str) -> list[str]:\n",
    "    tokens = []\n",
    "    while s:\n",
    "        if s[0] == '\\\\':\n",
    "            tokens.append(_COMMAND_RE.match(s).group(0))\n",
    "        else:\n",
    "            tokens.append(s[0])\n",
    "        s = s[len(tokens[-1]):]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Example\n",
    "print(tokenize_expression(r'\\frac{\\alpha}{2}\\not\\in\\mathbb{R}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:15.006113Z",
     "iopub.status.busy": "2024-12-20T10:10:15.005871Z",
     "iopub.status.idle": "2024-12-20T10:10:15.046173Z",
     "shell.execute_reply": "2024-12-20T10:10:15.045359Z",
     "shell.execute_reply.started": "2024-12-20T10:10:15.006089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import jiwer\n",
    "     \n",
    "\n",
    "def compute_cer(truth_and_output: list[tuple[str, str]]):\n",
    "  \"\"\"Computes CER given pairs of ground truth and model output.\"\"\"\n",
    "  class TokenizeTransform(jiwer.transforms.AbstractTransform):\n",
    "    def process_string(self, s: str):\n",
    "      return tokenize_expression(r'{}'.format(s))\n",
    "\n",
    "    def process_list(self, tokens: list[str]):\n",
    "      return [self.process_string(token) for token in tokens]\n",
    "\n",
    "  ground_truth, model_output = zip(*truth_and_output)\n",
    "\n",
    "  return jiwer.cer(truth=list(ground_truth),\n",
    "            hypothesis=list(model_output),\n",
    "            reference_transform=TokenizeTransform(),\n",
    "            hypothesis_transform=TokenizeTransform(),\n",
    "      )\n",
    "     \n",
    "\n",
    "# Test data to run compute_cer().\n",
    "# The first element is the model prediction, the second the ground truth.\n",
    "examples = [\n",
    "    (r'\\sqrt{2}', r'\\sqrt{2}'),  # 0 mistakes, 4 tokens\n",
    "    (r'\\frac{1}{2}', r'\\frac{i}{2}'),  # 1 mistake, 7 tokens\n",
    "    (r'\\alpha^{2}', 'a^{2}'),  # 1 mistake, 5 tokens\n",
    "    ('abc', 'def'),  # 3 mistakes, 3 tokens\n",
    "]\n",
    "\n",
    "# 5 mistakes for 19 tokens: 26.3% error rate.\n",
    "print(f\"{compute_cer(examples)*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:15.047649Z",
     "iopub.status.busy": "2024-12-20T10:10:15.047303Z",
     "iopub.status.idle": "2024-12-20T10:10:15.054852Z",
     "shell.execute_reply": "2024-12-20T10:10:15.053810Z",
     "shell.execute_reply.started": "2024-12-20T10:10:15.047610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#config\n",
    "\n",
    "common_config = {\n",
    "    'train_images_dirs': ['./train_splits/folder_001',\n",
    "                          './train_splits/folder_002',\n",
    "                          './train_splits/folder_003',\n",
    "                          './train_splits/folder_004',\n",
    "                          './train_splits/folder_005',\n",
    "                          './train_splits/folder_006',\n",
    "                          './train_splits/folder_007',\n",
    "                          './train_splits/folder_008',\n",
    "                          './train_splits/folder_009',\n",
    "                          './train_splits/folder_010',],\n",
    "    'valid_images_dirs': ['./train_splits/folder_099'],\n",
    "    'labels_file': './train_splits/labels.json',\n",
    "    'img_width': 448,\n",
    "    'img_height': 336,\n",
    "    'map_to_seq_hidden': 64,\n",
    "    'rnn_hidden': 256,\n",
    "    'leaky_relu': False,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    'epochs': 50,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_batch_size': 32,\n",
    "    'lr': 1e-5,\n",
    "    'show_interval': 50,\n",
    "    'valid_interval': 300,\n",
    "    'save_interval': 300,\n",
    "    'cpu_workers': 4,\n",
    "    'reload_checkpoint': None,\n",
    "    'valid_max_iter': 100,\n",
    "    'decode_method': 'greedy',\n",
    "    'beam_size': 10,\n",
    "    'checkpoints_dir': '/kaggle/working/checkpoints/'\n",
    "}\n",
    "\n",
    "train_config.update(common_config)\n",
    "\n",
    "evaluate_config = {\n",
    "    'eval_batch_size': 512,\n",
    "    'cpu_workers': 4,\n",
    "    'reload_checkpoint': '/kaggle/output/crnn-pytorch/checkpoints/crnn_synth90k.pt',\n",
    "    'decode_method': 'beam_search',\n",
    "    'beam_size': 10,\n",
    "}\n",
    "\n",
    "evaluate_config.update(common_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:15.056165Z",
     "iopub.status.busy": "2024-12-20T10:10:15.055914Z",
     "iopub.status.idle": "2024-12-20T10:10:18.832058Z",
     "shell.execute_reply": "2024-12-20T10:10:18.831361Z",
     "shell.execute_reply.started": "2024-12-20T10:10:15.056142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#requirements\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp  # log(p1 + p2) = logsumexp([log_p1, log_p2])\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CTCLoss\n",
    "from tqdm import tqdm\n",
    "from docopt import docopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:18.834913Z",
     "iopub.status.busy": "2024-12-20T10:10:18.834540Z",
     "iopub.status.idle": "2024-12-20T10:10:18.847358Z",
     "shell.execute_reply": "2024-12-20T10:10:18.846534Z",
     "shell.execute_reply.started": "2024-12-20T10:10:18.834885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channel, img_height, img_width, num_class,\n",
    "                 map_to_seq_hidden=64, rnn_hidden=256, leaky_relu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn, (output_channel, output_height, output_width) = \\\n",
    "            self._cnn_backbone(img_channel, img_height, img_width, leaky_relu)\n",
    "        self.map_to_seq = nn.Linear(output_channel * output_height, map_to_seq_hidden)\n",
    "        self.rnn1 = nn.LSTM(map_to_seq_hidden, rnn_hidden, bidirectional=True)\n",
    "        self.rnn2 = nn.LSTM(2 * rnn_hidden, rnn_hidden, bidirectional=True)\n",
    "        self.dense = nn.Linear(2 * rnn_hidden, num_class)\n",
    "\n",
    "    def _cnn_backbone(self, img_channel, img_height, img_width, leaky_relu):\n",
    "        assert img_height % 16 == 0\n",
    "        assert img_width % 4 == 0\n",
    "\n",
    "        channels = [img_channel, 64, 128, 256, 256, 512, 512, 512]\n",
    "        kernel_sizes = [3, 3, 3, 3, 3, 3, 2]\n",
    "        strides = [1, 1, 1, 1, 1, 1, 1]\n",
    "        paddings = [1, 1, 1, 1, 1, 1, 0]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def conv_relu(i, batch_norm=False):\n",
    "            # shape of input: (batch, input_channel, height, width)\n",
    "            input_channel = channels[i]\n",
    "            output_channel = channels[i+1]\n",
    "\n",
    "            cnn.add_module(\n",
    "                f'conv{i}',\n",
    "                nn.Conv2d(input_channel, output_channel, kernel_sizes[i], strides[i], paddings[i])\n",
    "            )\n",
    "\n",
    "            if batch_norm:\n",
    "                cnn.add_module(f'batchnorm{i}', nn.BatchNorm2d(output_channel))\n",
    "\n",
    "            relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n",
    "            cnn.add_module(f'relu{i}', relu)\n",
    "\n",
    "        # size of image: (channel, height, width) = (img_channel, img_height, img_width)\n",
    "        conv_relu(0)\n",
    "        cnn.add_module('pooling0', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # (64, img_height // 2, img_width // 2)\n",
    "\n",
    "        conv_relu(1)\n",
    "        cnn.add_module('pooling1', nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # (128, img_height // 4, img_width // 4)\n",
    "\n",
    "        conv_relu(2)\n",
    "        conv_relu(3)\n",
    "        cnn.add_module(\n",
    "            'pooling2',\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )  # (256, img_height // 8, img_width // 4)\n",
    "\n",
    "        conv_relu(4, batch_norm=True)\n",
    "        conv_relu(5, batch_norm=True)\n",
    "        cnn.add_module(\n",
    "            'pooling3',\n",
    "            nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )  # (512, img_height // 16, img_width // 4)\n",
    "\n",
    "        conv_relu(6)  # (512, img_height // 16 - 1, img_width // 4 - 1)\n",
    "\n",
    "        output_channel, output_height, output_width = \\\n",
    "            channels[-1], img_height // 16 - 1, img_width // 4 - 1\n",
    "        return cnn, (output_channel, output_height, output_width)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # shape of images: (batch, channel, height, width)\n",
    "\n",
    "        conv = self.cnn(images)\n",
    "        batch, channel, height, width = conv.size()\n",
    "\n",
    "        conv = conv.view(batch, channel * height, width)\n",
    "        conv = conv.permute(2, 0, 1)  # (width, batch, feature)\n",
    "        seq = self.map_to_seq(conv)\n",
    "\n",
    "        recurrent, _ = self.rnn1(seq)\n",
    "        recurrent, _ = self.rnn2(recurrent)\n",
    "\n",
    "        output = self.dense(recurrent)\n",
    "        return output  # shape: (seq_len, batch, num_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:18.848988Z",
     "iopub.status.busy": "2024-12-20T10:10:18.848699Z",
     "iopub.status.idle": "2024-12-20T10:10:18.865516Z",
     "shell.execute_reply": "2024-12-20T10:10:18.864659Z",
     "shell.execute_reply.started": "2024-12-20T10:10:18.848962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#ctc\n",
    "\n",
    "NINF = -1 * float('inf')\n",
    "DEFAULT_EMISSION_THRESHOLD = 0.01\n",
    "\n",
    "\n",
    "def _reconstruct(labels, blank=0):\n",
    "    new_labels = []\n",
    "    # merge same labels\n",
    "    previous = None\n",
    "    for l in labels:\n",
    "        if l != previous:\n",
    "            new_labels.append(l)\n",
    "            previous = l\n",
    "    # delete blank\n",
    "    new_labels = [l for l in new_labels if l != blank]\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def greedy_decode(emission_log_prob, blank=0, **kwargs):\n",
    "    labels = np.argmax(emission_log_prob, axis=-1)\n",
    "    labels = _reconstruct(labels, blank=blank)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def beam_search_decode(emission_log_prob, blank=0, **kwargs):\n",
    "    beam_size = kwargs['beam_size']\n",
    "    emission_threshold = kwargs.get('emission_threshold', np.log(DEFAULT_EMISSION_THRESHOLD))\n",
    "\n",
    "    length, class_count = emission_log_prob.shape\n",
    "\n",
    "    beams = [([], 0)]  # (prefix, accumulated_log_prob)\n",
    "    for t in range(length):\n",
    "        new_beams = []\n",
    "        for prefix, accumulated_log_prob in beams:\n",
    "            for c in range(class_count):\n",
    "                log_prob = emission_log_prob[t, c]\n",
    "                if log_prob < emission_threshold:\n",
    "                    continue\n",
    "                new_prefix = prefix + [c]\n",
    "                # log(p1 * p2) = log_p1 + log_p2\n",
    "                new_accu_log_prob = accumulated_log_prob + log_prob\n",
    "                new_beams.append((new_prefix, new_accu_log_prob))\n",
    "\n",
    "        # sorted by accumulated_log_prob\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = new_beams[:beam_size]\n",
    "\n",
    "    # sum up beams to produce labels\n",
    "    total_accu_log_prob = {}\n",
    "    for prefix, accu_log_prob in beams:\n",
    "        labels = tuple(_reconstruct(prefix, blank))\n",
    "        # log(p1 + p2) = logsumexp([log_p1, log_p2])\n",
    "        total_accu_log_prob[labels] = \\\n",
    "            logsumexp([accu_log_prob, total_accu_log_prob.get(labels, NINF)])\n",
    "\n",
    "    labels_beams = [(list(labels), accu_log_prob)\n",
    "                    for labels, accu_log_prob in total_accu_log_prob.items()]\n",
    "    labels_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "    labels = labels_beams[0][0]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def prefix_beam_decode(emission_log_prob, blank=0, **kwargs):\n",
    "    beam_size = kwargs['beam_size']\n",
    "    emission_threshold = kwargs.get('emission_threshold', np.log(DEFAULT_EMISSION_THRESHOLD))\n",
    "\n",
    "    length, class_count = emission_log_prob.shape\n",
    "\n",
    "    beams = [(tuple(), (0, NINF))]  # (prefix, (blank_log_prob, non_blank_log_prob))\n",
    "    # initial of beams: (empty_str, (log(1.0), log(0.0)))\n",
    "\n",
    "    for t in range(length):\n",
    "        new_beams_dict = defaultdict(lambda: (NINF, NINF))  # log(0.0) = NINF\n",
    "\n",
    "        for prefix, (lp_b, lp_nb) in beams:\n",
    "            for c in range(class_count):\n",
    "                log_prob = emission_log_prob[t, c]\n",
    "                if log_prob < emission_threshold:\n",
    "                    continue\n",
    "\n",
    "                end_t = prefix[-1] if prefix else None\n",
    "\n",
    "                # if new_prefix == prefix\n",
    "                new_lp_b, new_lp_nb = new_beams_dict[prefix]\n",
    "\n",
    "                if c == blank:\n",
    "                    new_beams_dict[prefix] = (\n",
    "                        logsumexp([new_lp_b, lp_b + log_prob, lp_nb + log_prob]),\n",
    "                        new_lp_nb\n",
    "                    )\n",
    "                    continue\n",
    "                if c == end_t:\n",
    "                    new_beams_dict[prefix] = (\n",
    "                        new_lp_b,\n",
    "                        logsumexp([new_lp_nb, lp_nb + log_prob])\n",
    "                    )\n",
    "\n",
    "                # if new_prefix == prefix + (c,)\n",
    "                new_prefix = prefix + (c,)\n",
    "                new_lp_b, new_lp_nb = new_beams_dict[new_prefix]\n",
    "\n",
    "                if c != end_t:\n",
    "                    new_beams_dict[new_prefix] = (\n",
    "                        new_lp_b,\n",
    "                        logsumexp([new_lp_nb, lp_b + log_prob, lp_nb + log_prob])\n",
    "                    )\n",
    "                else:\n",
    "                    new_beams_dict[new_prefix] = (\n",
    "                        new_lp_b,\n",
    "                        logsumexp([new_lp_nb, lp_b + log_prob])\n",
    "                    )\n",
    "\n",
    "        # sorted by log(blank_prob + non_blank_prob)\n",
    "        beams = sorted(new_beams_dict.items(), key=lambda x: logsumexp(x[1]), reverse=True)\n",
    "        beams = beams[:beam_size]\n",
    "\n",
    "    labels = list(beams[0][0])\n",
    "    return labels\n",
    "\n",
    "\n",
    "def ctc_decode(log_probs, label2char=None, blank=0, method='beam_search', beam_size=10):\n",
    "    emission_log_probs = np.transpose(log_probs.cpu().numpy(), (1, 0, 2))\n",
    "    # size of emission_log_probs: (batch, length, class)\n",
    "\n",
    "    decoders = {\n",
    "        'greedy': greedy_decode,\n",
    "        'beam_search': beam_search_decode,\n",
    "        'prefix_beam_search': prefix_beam_decode,\n",
    "    }\n",
    "    decoder = decoders[method]\n",
    "\n",
    "    decoded_list = []\n",
    "    for emission_log_prob in emission_log_probs:\n",
    "        decoded = decoder(emission_log_prob, blank=blank, beam_size=beam_size)\n",
    "        if label2char:\n",
    "            decoded = [label2char[l] for l in decoded]\n",
    "        decoded_list.append(decoded)\n",
    "\n",
    "    return decoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:18.867077Z",
     "iopub.status.busy": "2024-12-20T10:10:18.866822Z",
     "iopub.status.idle": "2024-12-20T10:10:18.885337Z",
     "shell.execute_reply": "2024-12-20T10:10:18.884639Z",
     "shell.execute_reply.started": "2024-12-20T10:10:18.867052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#dataset\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "class LatexOcrDataset(Dataset):\n",
    "\n",
    "    CHARS = ['_', '^', '{', '}', '&', '\\\\\\\\', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
    "             's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\\\mathbb{A}', '\\\\mathbb{B}',\n",
    "             '\\\\mathbb{C}', '\\\\mathbb{D}', '\\\\mathbb{E}', '\\\\mathbb{F}', '\\\\mathbb{G}', '\\\\mathbb{H}', '\\\\mathbb{I}', '\\\\mathbb{J}',\n",
    "             '\\\\mathbb{K}', '\\\\mathbb{L}', '\\\\mathbb{M}', '\\\\mathbb{N}', '\\\\mathbb{O}', '\\\\mathbb{P}', '\\\\mathbb{Q}', '\\\\mathbb{R}',\n",
    "             '\\\\mathbb{S}', '\\\\mathbb{T}', '\\\\mathbb{U}', '\\\\mathbb{V}', '\\\\mathbb{W}', '\\\\mathbb{X}', '\\\\mathbb{Y}', '\\\\mathbb{Z}',\n",
    "             '\\\\mathbb', ',', ';', ':', '!', '?', '.', '(', ')', '[', ']', '\\\\{', '\\\\}', '*', '/', '+', '-', '\\\\_', '\\\\&', '\\\\#', '\\\\%', '|',\n",
    "             '\\\\backslash', '\\\\alpha', '\\\\beta', '\\\\delta', '\\\\Delta', '\\\\epsilon', '\\\\eta', '\\\\chi', '\\\\gamma', '\\\\Gamma', '\\\\iota',\n",
    "             '\\\\kappa', '\\\\lambda', '\\\\Lambda', '\\\\nu', '\\\\mu', '\\\\omega', '\\\\Omega', '\\\\phi', '\\\\Phi', '\\\\pi', '\\\\Pi', '\\\\psi', '\\\\Psi',\n",
    "             '\\\\rho', '\\\\sigma', '\\\\Sigma', '\\\\tau', '\\\\theta', '\\\\Theta', '\\\\upsilon', '\\\\Upsilon', '\\\\varphi', '\\\\varpi', '\\\\varsigma',\n",
    "             '\\\\vartheta', '\\\\xi', '\\\\Xi', '\\\\zeta', '\\\\frac', '\\\\sqrt', '\\\\prod', '\\\\sum', '\\\\iint', '\\\\int', '\\\\oint', '\\\\hat', '\\\\tilde',\n",
    "             '\\\\vec', '\\\\overline', '\\\\underline', '\\\\prime', '\\\\dot', '\\\\not', '\\\\begin{matrix}', '\\\\end{matrix}', '\\\\langle', '\\\\rangle',\n",
    "             '\\\\lceil', '\\\\rceil', '\\\\lfloor', '\\\\rfloor', '\\\\|', '\\\\ge', '\\\\gg', '\\\\le', '\\\\ll', '<', '>', '=', '\\\\approx', '\\\\cong', '\\\\equiv',\n",
    "             '\\\\ne', '\\\\propto', '\\\\sim', '\\\\simeq', '\\\\in', '\\\\ni', '\\\\notin', '\\\\sqsubseteq', '\\\\subset', '\\\\subseteq', '\\\\subsetneq',\n",
    "             '\\\\supset', '\\\\supseteq', '\\\\emptyset', '\\\\times', '\\\\bigcap', '\\\\bigcirc', '\\\\bigcup', '\\\\bigoplus', '\\\\bigvee', '\\\\bigwedge',\n",
    "             '\\\\cap', '\\\\cup', '\\\\div', '\\\\mp', '\\\\odot', '\\\\ominus', '\\\\oplus', '\\\\otimes', '\\\\pm', '\\\\vee', '\\\\wedge', '\\\\hookrightarrow',\n",
    "             '\\\\leftarrow', '\\\\leftrightarrow', '\\\\Leftrightarrow', '\\\\longrightarrow', '\\\\mapsto', '\\\\rightarrow', '\\\\Rightarrow',\n",
    "             '\\\\rightleftharpoons', '\\\\iff', '\\\\bullet', '\\\\cdot', '\\\\circ', '\\\\aleph', '\\\\angle', '\\\\dagger', '\\\\exists', '\\\\forall',\n",
    "             '\\\\hbar', '\\\\infty', '\\\\models', '\\\\nabla', '\\\\neg', '\\\\partial', '\\\\perp', '\\\\top', '\\\\triangle', '\\\\triangleleft',\n",
    "             '\\\\triangleq', '\\\\vdash', '\\\\Vdash', '\\\\vdots']  \n",
    "    CHAR2LABEL = {char: i + 1 for i, char in enumerate(CHARS)}\n",
    "    LABEL2CHAR = {label: char for char, label in CHAR2LABEL.items()}\n",
    "    _COMMAND_RE = re.compile(r'\\\\(mathbb{[a-zA-Z]}|begin{[a-z]+}|end{[a-z]+}|operatorname\\*|[a-zA-Z]+|.)')\n",
    "    \n",
    "    def tokenize_expression(self, s: str) -> list[str]:\n",
    "        tokens = []\n",
    "        while s:\n",
    "            if s[0] == '\\\\':\n",
    "                tokens.append(self._COMMAND_RE.match(s).group(0))\n",
    "            else:\n",
    "                tokens.append(s[0])\n",
    "            s = s[len(tokens[-1]):]\n",
    "        return tokens\n",
    "\n",
    "    def __init__(self, images_dirs=None, labels_file=None, paths=None, mode=None, img_height=32, img_width=100):\n",
    "\n",
    "        if not images_dirs and not labels_file and paths:\n",
    "            texts = None\n",
    "        else:\n",
    "            paths, texts = self._load_from_raw_files(images_dirs, labels_file)\n",
    "        \n",
    "        self.paths = paths\n",
    "        self.texts = texts\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "\n",
    "    def _load_from_raw_files(self, images_dirs, labels_file):\n",
    "\n",
    "        paths = []\n",
    "        texts = []\n",
    "\n",
    "        with open(labels_file, 'r') as file:\n",
    "            labels = json.load(file)\n",
    "\n",
    "        for images_dir in images_dirs:\n",
    "            for filename in os.listdir(images_dir):\n",
    "                image_path = os.path.join(images_dir, filename)\n",
    "                image_id = filename.split('.')[0]\n",
    "                paths.append(image_path)\n",
    "                texts.append(labels[image_id])\n",
    "        \n",
    "        return paths, texts\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(path).convert('L')  # grey-scale\n",
    "        except IOError:\n",
    "            print('Corrupted image for %d' % index)\n",
    "            return self[index + 1]\n",
    "\n",
    "        image = image.resize((self.img_width, self.img_height), resample=Image.BILINEAR)\n",
    "        image = np.array(image)\n",
    "        image = image.reshape((1, self.img_height, self.img_width))\n",
    "        image = (image / 127.5) - 1.0\n",
    "        image = torch.FloatTensor(image)\n",
    "        \n",
    "        if self.texts:\n",
    "            text = self.texts[index]\n",
    "            tokens = self.tokenize_expression(text)\n",
    "            target = [self.CHAR2LABEL[t] for t in tokens]\n",
    "            target_length = [len(target)]\n",
    "            target = torch.LongTensor(target)\n",
    "            target_length = torch.LongTensor(target_length)\n",
    "            return image, target, target_length\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "def latex_ocr_collate_fn(batch):\n",
    "    images, targets, target_lengths = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    targets = torch.cat(targets, 0)\n",
    "    target_lengths = torch.cat(target_lengths, 0)\n",
    "    return images, targets, target_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:18.886942Z",
     "iopub.status.busy": "2024-12-20T10:10:18.886698Z",
     "iopub.status.idle": "2024-12-20T10:10:18.904307Z",
     "shell.execute_reply": "2024-12-20T10:10:18.903611Z",
     "shell.execute_reply.started": "2024-12-20T10:10:18.886918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "def evaluate(crnn, dataloader, criterion,\n",
    "             max_iter=None, decode_method='beam_search', beam_size=10):\n",
    "    crnn.eval()\n",
    "\n",
    "    tot_count = 0\n",
    "    tot_loss = 0\n",
    "    tot_correct = 0\n",
    "    wrong_cases = []\n",
    "\n",
    "    # Print(\"Evaluating\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            if max_iter and i >= max_iter:\n",
    "                break\n",
    "            device = 'cuda' if next(crnn.parameters()).is_cuda else 'cpu'\n",
    "\n",
    "            images, targets, target_lengths = [d.to(device) for d in data]\n",
    "\n",
    "            logits = crnn(images)\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            input_lengths = torch.LongTensor([logits.size(0)] * batch_size)\n",
    "\n",
    "            loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "\n",
    "            preds = ctc_decode(log_probs, method=decode_method, beam_size=beam_size)\n",
    "            reals = targets.cpu().numpy().tolist()\n",
    "            target_lengths = target_lengths.cpu().numpy().tolist()\n",
    "\n",
    "            tot_count += batch_size\n",
    "            tot_loss += loss.item()\n",
    "            target_length_counter = 0\n",
    "            for pred, target_length in zip(preds, target_lengths):\n",
    "                real = reals[target_length_counter:target_length_counter + target_length]\n",
    "                target_length_counter += target_length\n",
    "                if pred == real:\n",
    "                    tot_correct += 1\n",
    "                else:\n",
    "                    wrong_cases.append((real, pred))\n",
    "\n",
    "\n",
    "    evaluation = {\n",
    "        'loss': tot_loss / tot_count,\n",
    "        'acc': tot_correct / tot_count,\n",
    "        'wrong_cases': wrong_cases\n",
    "    }\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "def eval_main():\n",
    "    config = evaluate_config\n",
    "    eval_batch_size = config['eval_batch_size']\n",
    "    cpu_workers = config['cpu_workers']\n",
    "    reload_checkpoint = config['reload_checkpoint']\n",
    "\n",
    "    img_height = config['img_height']\n",
    "    img_width = config['img_width']\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'device: {device}')\n",
    "\n",
    "    test_dataset = LatexOcrDataset(images_dirs=config['train_images_dirs'], labels_file=config['labels_file'],\n",
    "                                   img_height=img_height, img_width=img_width)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cpu_workers,\n",
    "        collate_fn=latex_ocr_collate_fn)\n",
    "\n",
    "    num_class = len(LatexOcrDataset.LABEL2CHAR) + 1\n",
    "    crnn = CRNN(1, img_height, img_width, num_class,\n",
    "                map_to_seq_hidden=config['map_to_seq_hidden'],\n",
    "                rnn_hidden=config['rnn_hidden'],\n",
    "                leaky_relu=config['leaky_relu'])\n",
    "    crnn.load_state_dict(torch.load(reload_checkpoint, map_location=device))\n",
    "    crnn.to(device)\n",
    "\n",
    "    criterion = CTCLoss(reduction='sum')\n",
    "    criterion.to(device)\n",
    "\n",
    "    evaluation = evaluate(crnn, test_loader, criterion,\n",
    "                          decode_method=config['decode_method'],\n",
    "                          beam_size=config['beam_size'])\n",
    "    print('test_evaluation: loss={loss}, acc={acc}'.format(**evaluation))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     eval_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:18.905694Z",
     "iopub.status.busy": "2024-12-20T10:10:18.905414Z",
     "iopub.status.idle": "2024-12-20T10:10:18.922967Z",
     "shell.execute_reply": "2024-12-20T10:10:18.922196Z",
     "shell.execute_reply.started": "2024-12-20T10:10:18.905669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "\n",
    "def train_batch(crnn, data, optimizer, criterion, device):\n",
    "    crnn.train()\n",
    "    images, targets, target_lengths = [d.to(device) for d in data]\n",
    "\n",
    "    logits = crnn(images)\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
    "\n",
    "    batch_size = images.size(0)\n",
    "    input_lengths = torch.LongTensor([logits.size(0)] * batch_size)\n",
    "    target_lengths = torch.flatten(target_lengths)\n",
    "\n",
    "    loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(crnn.parameters(), 5) # gradient clipping with 5\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train():\n",
    "    config = train_config\n",
    "    print(config)\n",
    "    epochs = config['epochs']\n",
    "    train_batch_size = config['train_batch_size']\n",
    "    eval_batch_size = config['eval_batch_size']\n",
    "    lr = config['lr']\n",
    "    show_interval = config['show_interval']\n",
    "    valid_interval = config['valid_interval']\n",
    "    save_interval = config['save_interval']\n",
    "    cpu_workers = config['cpu_workers']\n",
    "    reload_checkpoint = config['reload_checkpoint']\n",
    "    valid_max_iter = config['valid_max_iter']\n",
    "\n",
    "    img_width = config['img_width']\n",
    "    img_height = config['img_height']\n",
    "    train_images_dirs = config['train_images_dirs']\n",
    "    valid_images_dirs = config['valid_images_dirs']\n",
    "    labels_file = config['labels_file']\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'device: {device}')\n",
    "\n",
    "    train_dataset = LatexOcrDataset(images_dirs=train_images_dirs, labels_file=labels_file,\n",
    "                                    img_height=img_height, img_width=img_width)\n",
    "    valid_dataset = LatexOcrDataset(images_dirs=valid_images_dirs, labels_file=labels_file,\n",
    "                                    img_height=img_height, img_width=img_width)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cpu_workers,\n",
    "        collate_fn=latex_ocr_collate_fn)\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cpu_workers,\n",
    "        collate_fn=latex_ocr_collate_fn)\n",
    "\n",
    "    num_class = len(LatexOcrDataset.LABEL2CHAR) + 1\n",
    "    crnn = CRNN(1, img_height, img_width, num_class,\n",
    "                map_to_seq_hidden=config['map_to_seq_hidden'],\n",
    "                rnn_hidden=config['rnn_hidden'],\n",
    "                leaky_relu=config['leaky_relu'])\n",
    "    if reload_checkpoint:\n",
    "        print(reload_checkpoint)\n",
    "        crnn.load_state_dict(torch.load(reload_checkpoint, map_location=device))\n",
    "    crnn.to(device)\n",
    "\n",
    "    optimizer = optim.RMSprop(crnn.parameters(), lr=lr)\n",
    "    criterion = CTCLoss(reduction='sum', zero_infinity=True)\n",
    "    criterion.to(device)\n",
    "\n",
    "    assert save_interval % valid_interval == 0\n",
    "\n",
    "\n",
    "    loss_list = []\n",
    "    previous_loss = 0.999999999999\n",
    "    loss_list.append(previous_loss)\n",
    "    i = 1\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f'epoch: {epoch}')\n",
    "        tot_train_loss = 0.\n",
    "        tot_train_count = 0\n",
    "        for train_data in tqdm(train_loader, desc=\"Training\"):\n",
    "            loss = train_batch(crnn, train_data, optimizer, criterion, device)\n",
    "            train_size = train_data[0].size(0)\n",
    "            tot_train_loss += loss\n",
    "            tot_train_count += train_size\n",
    "            if i % show_interval == 0:\n",
    "                print('train_batch_loss[', i, ']: ', loss / train_size)\n",
    "\n",
    "            if i % valid_interval == 0:\n",
    "                evaluation = evaluate(crnn, valid_loader, criterion,\n",
    "                                      decode_method=config['decode_method'],\n",
    "                                      beam_size=config['beam_size'])\n",
    "                print('valid_evaluation: loss={loss}, acc={acc}'.format(**evaluation))\n",
    "\n",
    "                if i % save_interval == 0:\n",
    "                    prefix = 'crnn'\n",
    "                    loss = evaluation['loss']\n",
    "                    # Ensure the directory exists\n",
    "                    os.makedirs(config['checkpoints_dir'], exist_ok=True)\n",
    "                    save_model_path = os.path.join(config['checkpoints_dir'],\n",
    "                                                   f'{prefix}_{i:06}_loss{loss}.pt')\n",
    "                    torch.save(crnn.state_dict(), save_model_path)\n",
    "                    print('save model at ', save_model_path)\n",
    "                    reload_checkpoint = save_model_path\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        current_loss = tot_train_loss / tot_train_count\n",
    "        loss_list.append(current_loss)\n",
    "        print('train_loss: ', current_loss)\n",
    "\n",
    "\n",
    "    \n",
    "    # Save the training loss\n",
    "    file_path = '/kaggle/working/loss_list.txt'\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Write each loss value to the file\n",
    "        for loss in loss_list:\n",
    "            file.write(f\"{loss}\\n\")\n",
    "    \n",
    "    print(f\"Loss list saved to {file_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:10:18.924119Z",
     "iopub.status.busy": "2024-12-20T10:10:18.923852Z",
     "iopub.status.idle": "2024-12-20T10:16:07.306456Z",
     "shell.execute_reply": "2024-12-20T10:16:07.305041Z",
     "shell.execute_reply.started": "2024-12-20T10:10:18.924094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6002753,
     "sourceId": 9795294,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6122228,
     "sourceId": 9954554,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6219762,
     "sourceId": 10087528,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 161792,
     "modelInstanceId": 148619,
     "sourceId": 174559,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
